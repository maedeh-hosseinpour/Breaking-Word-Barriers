{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuR01inMvJhE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gspread\n",
        "from google.colab import auth\n",
        "\n",
        "from google.auth import default\n",
        "from gspread_dataframe import get_as_dataframe\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "!pip install wordfreq\n",
        "from wordfreq import zipf_frequency\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "import seaborn as sns\n",
        "from scipy.stats import fisher_exact\n",
        "from statsmodels.stats.multitest import multipletests\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "s6__SP6HvSL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading files\n",
        "words = gc.open_by_url('https://docs.google.com/spreadsheets/d/1mDg1HuTNgquVK4a9T5Nu--21W0NJzsSPASMfl6A1gCo/edit?gid=0#gid=0')\n",
        "pre = gc.open_by_url('https://docs.google.com/spreadsheets/d/1EvC0PwMzbipWXdxIbCXc6urVQZjDJV4oRNRG68WaWnQ/edit?resourcekey=&gid=974437907#gid=974437907')\n",
        "post = gc.open_by_url('https://docs.google.com/spreadsheets/d/1rl_UsiBDM7flcRjVeB4HtysQkVA1M3fwJmDx7Ct1Cp0/edit?resourcekey=&gid=1754048364#gid=1754048364')\n",
        "\n",
        "wordsheet = words.worksheet('Sheet1')\n",
        "presheet = pre.worksheet('Sheet1')\n",
        "postsheet = post.worksheet('Sheet1')\n",
        "\n",
        "word = get_as_dataframe(wordsheet)\n",
        "pre = get_as_dataframe(presheet)\n",
        "post = get_as_dataframe(postsheet)\n",
        "\n",
        "url = \"/content/drive/MyDrive/Colab Notebooks/challenging_words_zipf.xlsx\"\n",
        "word_dataset = pd.read_excel(url)\n",
        "\n",
        "medical_dys = gc.open_by_url('https://docs.google.com/spreadsheets/d/13W3FAc_f1tfZU3SXAKlBl3cyKoqPIWtHNJh3J1jm3t8/edit?gid=1533757235#gid=1533757235')\n",
        "self_dys = gc.open_by_url('https://docs.google.com/spreadsheets/d/1Emyuq_kbaRgTJEDVRqF8dT2HA-L4ZsIs8SSzzuX0U7o/edit?gid=1428308089#gid=1428308089')\n",
        "\n",
        "medsheet = medical_dys.worksheet('Sheet1')\n",
        "selfsheet = self_dys.worksheet('Sheet1')\n",
        "\n",
        "medical_dyslexic = get_as_dataframe(medsheet)\n",
        "self_dyslexic = get_as_dataframe(selfsheet)\n",
        "\n",
        "\n",
        "# add a dyslexia score based on the screener to the word dataset\n",
        "\n",
        "pre = pre.rename(columns={'Please enter your Prolific ID:':'Prolific ID'})\n",
        "pre = pre.drop_duplicates(subset='Prolific ID')\n",
        "duplicate_ids = pre['Prolific ID'][pre['Prolific ID'].duplicated()]\n",
        "\n",
        "pre['dys-score'] = ((pre.iloc[:,13:18]=='Yes').sum(axis=1))*3 + ((pre.iloc[:,18:24]=='Yes').sum(axis=1))*2 + ((pre.iloc[:,24:25]=='Yes').sum(axis=1))*3\n",
        "word = word.merge(pre[['Prolific ID','dys-score']], on='Prolific ID', how='left' )\n",
        "\n",
        "word['dys-score'] = word['dys-score'].fillna(-1)\n",
        "word['dys-score'] = word['dys-score'].astype(int)\n",
        "\n",
        "word = word.drop_duplicates(keep='first')\n",
        "\n",
        "# add a column if they self-diagnosed themselves as dyslexic\n",
        "word = word.merge(pre[['Have you ever been diagnosed with Dyslexia?','Prolific ID']], on='Prolific ID', how='left' )\n",
        "word = word.rename(columns={'Have you ever been diagnosed with Dyslexia?':'self-diagnosed'})\n",
        "\n",
        "# rename columns and merge it with the challanging word dataset\n",
        "word_dataset = word_dataset.rename(columns={'word':'words'})\n",
        "word = word.merge(word_dataset, on='words', how='left')\n",
        "\n",
        "# extract the last difficulty of each word\n",
        "word['last_difficulty'] = word['difficulty'].fillna('').astype(str).str.split(',').str[-1]\n",
        "\n",
        "# extract ids from prolific demography datasets -> Project: dyslexia - Medical\n",
        "medical = medical_dyslexic['Participant id'].to_list()\n",
        "word['med_dys'] = word['Prolific ID'].isin(medical)\n",
        "\n",
        "\n",
        "# extract ids from prolific demography datasets -> Project: Dyslexia\n",
        "self_dyslex = self_dyslexic['Participant id'].to_list()\n",
        "word['self_dys_prolific'] = word['Prolific ID'].isin(self_dyslex)\n",
        "\n",
        "# remove unnecessary columns\n",
        "word = word.drop(columns=['difficulty', 'timeStamps', 'reason', 'url', 'Server Timestamp'])\n",
        "\n",
        "word = word[word['last_difficulty'].astype(str).str.strip() != '']\n",
        "word['Prolific ID'][word['Prolific ID']=='66d9a9dfafa9f033be6ee60a\\n']='66d9a9dfafa9f033be6ee60a'\n",
        "pre['What is your gender?'][pre['What is your gender?'].isna()]='unknown'\n",
        "pre['What is your age? (in years)'][pre['What is your gender?']=='unknown'] = 0\n",
        "\n",
        "\n",
        "def safe_zipf(word):\n",
        "    if isinstance(word, str):\n",
        "        return zipf_frequency(word, 'en')\n",
        "    else:\n",
        "        return None\n",
        "word['zipf'] = word['words'].apply(safe_zipf)\n",
        "\n",
        "# Data Cleaning\n",
        "pre['What is your gender?'] = pre['What is your gender?'].str.strip().str.lower()\n",
        "\n",
        "gender_map = {\n",
        "    'male': 'Male',\n",
        "    'female': 'Female',\n",
        "    'man': 'Male',\n",
        "    'woman': 'Female',\n",
        "    'nonbinary': 'Non-binary',\n",
        "    'non-binary': 'Non-binary',\n",
        "    'transgender': 'Transgender',\n",
        "    'm':'Male',\n",
        "    'femal':'Female',\n",
        "    'nonbinary transmasculine' : 'Non-binary',\n",
        "    'f':'Female',\n",
        "    'non binary' : 'Non-binary',\n",
        "    'transmasculine' : 'Transgender'\n",
        "}\n",
        "\n",
        "pre['What is your gender?'] = pre['What is your gender?'].map(gender_map).fillna(pre['What is your gender?'])\n",
        "\n",
        "pre['What is your gender?'].value_counts()\n",
        "\n",
        "\n",
        "# Data Integration (pre-survey and word dataset)\n",
        "print(pre['Prolific ID'].nunique(), word['Prolific ID'].nunique())\n",
        "\n",
        "pre_list = pre['Prolific ID'].to_list()\n",
        "word_list = word['Prolific ID'].to_list()\n",
        "\n",
        "temp = word[~word['Prolific ID'].isin(pre_list)]\n",
        "print(temp['Prolific ID'].nunique()) # must be 0\n",
        "\n",
        "pre = pre[pre['Prolific ID'].isin(word_list)]\n",
        "print(pre['Prolific ID'].nunique())\n",
        "\n",
        "\n",
        "# Remove under affected participants\n",
        "\n",
        "l = ['67ed8f12809e993b465ce944', '67ccc62a623f466a4ad19371']\n",
        "word = word[~word['Prolific ID'].isin(l)]\n",
        "print(word['Prolific ID'].nunique())\n"
      ],
      "metadata": {
        "id": "-5OA4ITRvWB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to datetime\n",
        "word['date and time'] = pd.to_datetime(word['date and time'])\n",
        "\n",
        "# Keep only participants with ≤ 55 ratings\n",
        "counts = word.groupby('Prolific ID').size()\n",
        "valid_ids = counts[counts <= 55].index\n",
        "filtered = word[word['Prolific ID'].isin(valid_ids)]\n",
        "\n",
        "# Identify participants who ONLY used \"1\" or \"2\"\n",
        "only_1_2_ids = filtered.groupby('Prolific ID')['last_difficulty'].apply(\n",
        "    lambda x: x.astype(str).str.strip().isin(['1','2']).all()\n",
        ")\n",
        "only_1_2_ids = only_1_2_ids[only_1_2_ids].index\n",
        "print('len', len(only_1_2_ids))\n",
        "\n",
        "\n",
        "# Compute durations for those participants\n",
        "durations = filtered.groupby('Prolific ID')['date and time'].max() - filtered.groupby('Prolific ID')['date and time'].min()\n",
        "durations_seconds = durations.dt.total_seconds()\n",
        "\n",
        "careless_durations = durations_seconds.loc[only_1_2_ids]\n",
        "\n",
        "# Compute cutoff = 25th percentile\n",
        "quartiles = careless_durations.quantile([0.25, 0.5, 0.75])\n",
        "cutoff = careless_durations.quantile(0.25)\n",
        "\n",
        "print(\"Quartiles of their session durations (seconds):\")\n",
        "print(quartiles)\n",
        "print(\"\\nCutoff (25th percentile of careless durations):\", cutoff, \"seconds ≈\", cutoff/60, \"minutes\")\n",
        "# print(careless_durations.sort_values())\n",
        "\n",
        "\n",
        "# Detect and remove the unreliable participants just selected 1\n",
        "df = word.groupby('Prolific ID')['last_difficulty']\n",
        "all_ones = df.apply(lambda s: (s == '1').all())\n",
        "all_ones.value_counts()\n",
        "num_participants_all_1 = int(all_ones.sum())\n",
        "ids_all_1 = all_ones[all_ones].index.tolist()\n",
        "\n",
        "print('only 1', num_participants_all_1)  # count and a peek at first 10 IDs\n",
        "\n",
        "# word_all1 = word[word['Prolific ID'].isin(ids_all_1)].copy()\n",
        "\n",
        "# Drop all rows from those participants\n",
        "word = word[~word['Prolific ID'].isin(ids_all_1)].copy()\n",
        "\n",
        "# Detect and remove the unreliable participants just selected 1 and 2 and did not take enough time\n",
        "df1 = word.copy()\n",
        "df1['last_difficulty'] = pd.to_numeric(df1['last_difficulty'], errors='coerce')\n",
        "df1['date and time'] = pd.to_datetime(df1['date and time'], errors='coerce')\n",
        "df1 = df1.dropna(subset=['last_difficulty', 'date and time'])\n",
        "\n",
        "df = df1.groupby('Prolific ID')\n",
        "\n",
        "only_12 = df['last_difficulty'].apply(lambda s: s.isin([1,2]).all())\n",
        "dur_min = (df['date and time'].max() - df['date and time'].min()).dt.total_seconds() / 60\n",
        "ids_3 = dur_min.index[only_12 & (dur_min < cutoff)].tolist()\n",
        "print('ids_1,2',len(ids_3))\n",
        "\n",
        "\n",
        "# Drop all rows from those participants\n",
        "word = word[~word['Prolific ID'].isin(ids_3)].copy()\n",
        "print(word['Prolific ID'].nunique())\n",
        "mask = (\n",
        "    (word['med_dys'] == True)|(word['dys-score'] >=12 ) )\n",
        "\n",
        "word_dys = word[mask]\n",
        "print('word_dys',word_dys['Prolific ID'].nunique())\n",
        "word_notdys = word[~mask]\n",
        "print('word_notdys',word_notdys['Prolific ID'].nunique())\n",
        "\n",
        "word = pd.concat([word_dys, word_notdys], ignore_index=True)\n",
        "print('word',word['Prolific ID'].nunique())\n",
        "\n",
        "\n",
        "# Only for non-teachers\n",
        "\n",
        "mask = (\n",
        "    (word['med_dys'] == True)|(word['dys-score'] >=12 ) )\n",
        "word_dys = word[mask]\n",
        "print(word_dys['Prolific ID'].nunique())\n",
        "word_notdys = word[~mask]\n",
        "print(word_notdys['Prolific ID'].nunique())\n",
        "dys_list= word_dys['Prolific ID'].to_list()\n",
        "nondys_list = word_notdys['Prolific ID'].to_list()\n",
        "\n",
        "word['dyslexia'] = False\n",
        "word['dyslexia'] = word['Prolific ID'].isin(dys_list)\n",
        "\n",
        "word = word.drop(columns=['self_dys_prolific','med_dys','self-diagnosed', 'familiarity', 'some letters', 'length'])\n",
        "word['dyslexia'].value_counts(dropna = False)\n",
        "\n",
        "\n",
        "pretemp = pre[['Prolific ID','What is your gender?','What is your age? (in years)']]\n",
        "wordtemp = word[['Prolific ID','dyslexia']]\n",
        "temp = wordtemp.merge(pretemp, on='Prolific ID', how='left')\n",
        "temp = temp.dropna()\n",
        "temp = temp.drop_duplicates(subset='Prolific ID')\n",
        "temp['What is your age? (in years)'] = temp['What is your age? (in years)'].astype(int)\n",
        "temp = temp[temp['What is your age? (in years)'] != 0]\n",
        "\n",
        "temp.groupby(['dyslexia', 'What is your gender?'])['What is your age? (in years)'].describe()"
      ],
      "metadata": {
        "id": "f91pPUZ8visB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalized distribution to see what word feature rated as easy for dyslexic people\n",
        "\n",
        "temp = word_dys.copy()\n",
        "\n",
        "\n",
        "rating_counts = temp.groupby('words')['last_difficulty'].value_counts().unstack(fill_value=0)\n",
        "rating_counts['total'] = rating_counts.sum(axis=1)\n",
        "\n",
        "#add columns of 1 ,2,3,4 to a new dataset: rating_counts\n",
        "rating_counts['1%'] = ((rating_counts['1'] / rating_counts['total']) * 100).round(2)\n",
        "rating_counts['2%'] = ((rating_counts['2'] / rating_counts['total']) * 100).round(2)\n",
        "rating_counts['3%'] = ((rating_counts['3'] / rating_counts['total']) * 100).round(2)\n",
        "rating_counts['4%'] = ((rating_counts['4'] / rating_counts['total']) * 100).round(2)\n",
        "\n",
        "rating_counts = rating_counts.merge(temp[['words','long_words', 'diagraphs', 'not_freq_words', 'silent_letters', 'vowel_digraphs','has_homophones', 'difficult_orthography']], on='words', how='left')\n",
        "rating_counts.drop_duplicates(subset='words', keep='first', inplace=True)\n",
        "# print(rating_counts.shape)\n",
        "# filter out the words that are rated as very easy with the higher rate than 80% and creating 2 new datasets: difficult_words, easy_words\n",
        "difficult_words = rating_counts[\n",
        "    ((rating_counts['total'] < 5) & (rating_counts['1']/ rating_counts['total']< 0.75)) |\n",
        "    ((rating_counts['total'] >= 5) & (rating_counts['1'] / rating_counts['total'] < 0.8))\n",
        "]\n",
        "dys_difficult_list = difficult_words['words'].to_list()\n",
        "# print(dys_difficult_list)\n",
        "\n",
        "easy_words = rating_counts[\n",
        "    ((rating_counts['total'] < 5) & (rating_counts['1']/ rating_counts['total']>= 0.75)) |\n",
        "    ((rating_counts['total'] >= 5) & (rating_counts['1'] / rating_counts['total'] >= 0.8))\n",
        "]\n",
        "\n",
        "# print(rating_counts.shape,difficult_words.shape, easy_words.shape)\n",
        "\n",
        "# Normalized distribution to see what word feature is the hardest(2,3,4) -  dyslexic people\n",
        "categories = ['long_words', 'diagraphs', 'not_freq_words',\n",
        "              'silent_letters', 'vowel_digraphs', 'has_homophones',\n",
        "              'difficult_orthography']\n",
        "\n",
        "normalized_stats = {}\n",
        "\n",
        "for cat in categories:\n",
        "    total_with_feature = rating_counts[rating_counts[cat] == 1].shape[0]\n",
        "    difficult_with_feature = difficult_words[difficult_words[cat] == 1].shape[0]\n",
        "\n",
        "    if total_with_feature > 0:\n",
        "        normalized_stats[cat] = round(difficult_with_feature / total_with_feature * 100, 2)\n",
        "    else:\n",
        "        normalized_stats[cat] = 0\n",
        "\n",
        "norm_dysdf = pd.DataFrame.from_dict(normalized_stats, orient='index', columns=['% Hard Ratings'])\n",
        "norm_dysdf = norm_dysdf.sort_values(by='% Hard Ratings', ascending=False)\n",
        "\n",
        "# Normalized distribution to see what word feature rated as easy for non-dyslexic people\n",
        "\n",
        "temp = word_notdys.copy()\n",
        "\n",
        "rating_counts = temp.groupby('words')['last_difficulty'].value_counts().unstack(fill_value=0)\n",
        "rating_counts['total'] = rating_counts.sum(axis=1)\n",
        "\n",
        "#add columns of 1 ,2,3,4 to a new dataset: rating_counts\n",
        "rating_counts['1%'] = ((rating_counts['1'] / rating_counts['total']) * 100).round(2)\n",
        "rating_counts['2%'] = ((rating_counts['2'] / rating_counts['total']) * 100).round(2)\n",
        "rating_counts['3%'] = ((rating_counts['3'] / rating_counts['total']) * 100).round(2)\n",
        "rating_counts['4%'] = ((rating_counts['4'] / rating_counts['total']) * 100).round(2)\n",
        "\n",
        "rating_counts = rating_counts.merge(temp[['words','long_words', 'diagraphs', 'not_freq_words', 'silent_letters', 'vowel_digraphs','has_homophones', 'difficult_orthography']], on='words', how='left')\n",
        "\n",
        "rating_counts.drop_duplicates(subset='words', keep='first', inplace=True)\n",
        "\n",
        "\n",
        "difficult_words = rating_counts[\n",
        "    ((rating_counts['total'] < 5) & (rating_counts['1']/ rating_counts['total']< 0.75)) |\n",
        "    ((rating_counts['total'] >= 5) & (rating_counts['1'] / rating_counts['total'] < 0.8))\n",
        "]\n",
        "nondys_difficult_list = difficult_words['words'].to_list()\n",
        "\n",
        "easy_words = rating_counts[\n",
        "    ((rating_counts['total'] < 5) & (rating_counts['1']/ rating_counts['total']>= 0.75)) |\n",
        "    ((rating_counts['total'] >= 5) & (rating_counts['1'] / rating_counts['total'] >= 0.8))\n",
        "]\n",
        "\n",
        "\n",
        "# Normalized distribution to see what word feature is the hardest - non dyslexic people\n",
        "categories = ['long_words', 'diagraphs', 'not_freq_words',\n",
        "              'silent_letters', 'vowel_digraphs', 'has_homophones',\n",
        "              'difficult_orthography']\n",
        "\n",
        "normalized_stats = {}\n",
        "\n",
        "for cat in categories:\n",
        "    total_with_feature = rating_counts[rating_counts[cat] == 1].shape[0]\n",
        "    difficult_with_feature = difficult_words[difficult_words[cat] == 1].shape[0]\n",
        "\n",
        "    if total_with_feature > 0:\n",
        "        normalized_stats[cat] = round(difficult_with_feature / total_with_feature * 100, 2)\n",
        "    else:\n",
        "        normalized_stats[cat] = 0\n",
        "\n",
        "norm_nondysdf = pd.DataFrame.from_dict(normalized_stats, orient='index', columns=['% Hard Ratings'])\n",
        "norm_nondysdf = norm_nondysdf.sort_values(by='% Hard Ratings', ascending=False)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XMIKCC6QyebZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBOOST\n",
        "word['dys hard'] = (word['words'].isin(dys_difficult_list)) & (word['dyslexia']==True).astype(int)\n",
        "word['nondys hard'] = (word['words'].isin(nondys_difficult_list)) & (word['dyslexia']==False).astype(int)\n",
        "\n",
        "dys_word = word[word['dyslexia']==True]\n",
        "nondys_word = word[word['dyslexia']==False]\n",
        "\n",
        "dys_word = dys_word.drop(columns=['nondys hard'])\n",
        "nondys_word = nondys_word.drop(columns=['dys hard'])\n",
        "\n",
        "\n",
        "dys_word['dys hard'] = dys_word['dys hard'].astype(int)\n",
        "nondys_word['nondys hard'] = nondys_word['nondys hard'].astype(int)\n",
        "\n",
        "dys_word1 = dys_word.drop(columns=['Prolific ID', 'words', 'letters', 'date and time', 'dys-score', 'last_difficulty','not_freq_words','dyslexia', 'diagraphs', 'not_freq_words', 'silent_letters', 'vowel_digraphs', 'has_homophones', 'difficult_orthography'])\n",
        "nondys_word1 = nondys_word.drop(columns=['Prolific ID', 'words', 'letters', 'date and time', 'dys-score', 'last_difficulty', 'not_freq_words','dyslexia', 'diagraphs', 'not_freq_words', 'silent_letters','vowel_digraphs', 'has_homophones', 'difficult_orthography'])\n",
        "dys_word1 = dys_word.drop(columns=['zipf'])\n",
        "nondys_word1 = nondys_word.drop(columns=['zipf'])\n",
        "dys_word1\n",
        "\n",
        "# Dyslexic dataset\n",
        "X_dys = dys_word1.drop(columns=['dys hard'])\n",
        "y_dys = dys_word1['dys hard']\n",
        "\n",
        "X_train_dys, X_test_dys, y_train_dys, y_test_dys = train_test_split(X_dys, y_dys, test_size=0.2, random_state=42)\n",
        "\n",
        "model_dys = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "model_dys.fit(X_train_dys, y_train_dys)\n",
        "\n",
        "y_pred_dys = model_dys.predict(X_test_dys)\n",
        "print(\"Accuracy (Dyslexic):\", accuracy_score(y_test_dys, y_pred_dys))\n",
        "\n",
        "#  Non-dyslexic dataset\n",
        "X_nondys = nondys_word1.drop(columns=['nondys hard'])\n",
        "y_nondys = nondys_word1['nondys hard']\n",
        "\n",
        "X_train_nondys, X_test_nondys, y_train_nondys, y_test_nondys = train_test_split(X_nondys, y_nondys, test_size=0.2, random_state=42)\n",
        "\n",
        "model_nondys = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "model_nondys.fit(X_train_nondys, y_train_nondys)\n",
        "\n",
        "y_pred_nondys = model_nondys.predict(X_test_nondys)\n",
        "\n",
        "\n",
        "#CV - baseline\n",
        "scoring = {\n",
        "    \"acc\": \"accuracy\",\n",
        "    \"f1\": \"f1\",\n",
        "    \"roc\": \"roc_auc\",\n",
        "    \"pr\": \"average_precision\",\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "clf = xgb.XGBClassifier(\n",
        "    tree_method=\"hist\",\n",
        "    eval_metric=\"logloss\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Dyslexic\n",
        "res_dys = cross_validate(clf, X_dys, y_dys, cv=cv, scoring=scoring, n_jobs=-1)\n",
        "print(\"Dyslexic (5-fold):\",\n",
        "      f\"Acc {np.mean(res_dys['test_acc']):.3f}\",\n",
        "      f\"F1 {np.mean(res_dys['test_f1']):.3f}\",\n",
        "      f\"ROC {np.mean(res_dys['test_roc']):.3f}\",\n",
        "      f\"PR {np.mean(res_dys['test_pr']):.3f}\")\n",
        "\n",
        "# Non-dyslexic\n",
        "res_nondys = cross_validate(clf, X_nondys, y_nondys, cv=cv, scoring=scoring, n_jobs=-1)\n",
        "print(\"Non-dyslexic (5-fold):\",\n",
        "      f\"Acc {np.mean(res_nondys['test_acc']):.3f}\",\n",
        "      f\"F1 {np.mean(res_nondys['test_f1']):.3f}\",\n",
        "      f\"ROC {np.mean(res_nondys['test_roc']):.3f}\",\n",
        "      f\"PR {np.mean(res_nondys['test_pr']):.3f}\")\n",
        "\n",
        "\n",
        "def mean_std(arr):\n",
        "    return f\"{np.mean(arr):.3f} ± {np.std(arr):.3f}\"\n",
        "\n",
        "print(\"Dyslexic (5-fold):\",\n",
        "      f\"Acc {mean_std(res_dys['test_acc'])}\",\n",
        "      f\"F1 {mean_std(res_dys['test_f1'])}\",\n",
        "      f\"ROC {mean_std(res_dys['test_roc'])}\",\n",
        "      f\"PR {mean_std(res_dys['test_pr'])}\")\n",
        "\n",
        "print(\"Non-dyslexic (5-fold):\",\n",
        "      f\"Acc {mean_std(res_nondys['test_acc'])}\",\n",
        "      f\"F1 {mean_std(res_nondys['test_f1'])}\",\n",
        "      f\"ROC {mean_std(res_nondys['test_roc'])}\",\n",
        "      f\"PR {mean_std(res_nondys['test_pr'])}\")"
      ],
      "metadata": {
        "id": "xj7acL6LyeLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost - seperate datasets\n",
        "\n",
        "dys_word['dys hard'] = dys_word['dys hard'].astype(int)\n",
        "nondys_word['nondys hard'] = nondys_word['nondys hard'].astype(int)\n",
        "\n",
        "\n",
        "\n",
        "# remove some columns\n",
        "dys_word = dys_word.drop(columns=['Prolific ID', 'words', 'letters', 'date and time', 'dys-score', 'last_difficulty','not_freq_words','dyslexia','reason explanation', 'letter explanation'])\n",
        "nondys_word = nondys_word.drop(columns=['Prolific ID', 'words', 'letters', 'date and time', 'dys-score', 'last_difficulty', 'not_freq_words','dyslexia','reason explanation', 'letter explanation'])\n",
        "dys_word1 = dys_word.drop(columns=['Prolific ID', 'words', 'letters', 'date and time', 'dys-score', 'last_difficulty','not_freq_words','dyslexia', 'diagraphs', 'not_freq_words', 'silent_letters', 'vowel_digraphs', 'has_homophones', 'difficult_orthography'])\n",
        "nondys_word1 = nondys_word.drop(columns=['Prolific ID', 'words', 'letters', 'date and time', 'dys-score', 'last_difficulty', 'not_freq_words','dyslexia', 'diagraphs', 'not_freq_words', 'silent_letters','vowel_digraphs', 'has_homophones', 'difficult_orthography'])\n",
        "dys_word1 = dys_word.drop(columns=['zipf'])\n",
        "nondys_word1 = nondys_word.drop(columns=['zipf'])\n",
        "dys_word1\n",
        "\n",
        "# Dyslexic dataset\n",
        "X_dys = dys_word1.drop(columns=['dys hard'])\n",
        "y_dys = dys_word1['dys hard']\n",
        "\n",
        "X_train_dys, X_test_dys, y_train_dys, y_test_dys = train_test_split(X_dys, y_dys, test_size=0.2, random_state=42)\n",
        "\n",
        "model_dys = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "model_dys.fit(X_train_dys, y_train_dys)\n",
        "\n",
        "y_pred_dys = model_dys.predict(X_test_dys)\n",
        "print(\"Accuracy (Dyslexic):\", accuracy_score(y_test_dys, y_pred_dys))\n",
        "\n",
        "#  Non-dyslexic dataset\n",
        "X_nondys = nondys_word1.drop(columns=['nondys hard'])\n",
        "y_nondys = nondys_word1['nondys hard']\n",
        "\n",
        "X_train_nondys, X_test_nondys, y_train_nondys, y_test_nondys = train_test_split(X_nondys, y_nondys, test_size=0.2, random_state=42)\n",
        "\n",
        "model_nondys = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "model_nondys.fit(X_train_nondys, y_train_nondys)\n",
        "\n",
        "y_pred_nondys = model_nondys.predict(X_test_nondys)\n",
        "print(\"Accuracy (Non-Dysleslexic):\", accuracy_score(y_test_nondys, y_pred_nondys))"
      ],
      "metadata": {
        "id": "ujxRXduD1bqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scoring = {\n",
        "    \"acc\": \"accuracy\",\n",
        "    \"f1\": \"f1\",\n",
        "    \"roc\": \"roc_auc\",\n",
        "    \"pr\": \"average_precision\",\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "clf = xgb.XGBClassifier(\n",
        "    tree_method=\"hist\",\n",
        "    eval_metric=\"logloss\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Dyslexic\n",
        "res_dys = cross_validate(clf, X_dys, y_dys, cv=cv, scoring=scoring, n_jobs=-1)\n",
        "print(\"Dyslexic (5-fold):\",\n",
        "      f\"Acc {np.mean(res_dys['test_acc']):.3f}\",\n",
        "      f\"F1 {np.mean(res_dys['test_f1']):.3f}\",\n",
        "      f\"ROC {np.mean(res_dys['test_roc']):.3f}\",\n",
        "      f\"PR {np.mean(res_dys['test_pr']):.3f}\")\n",
        "\n",
        "# Non-dyslexic\n",
        "res_nondys = cross_validate(clf, X_nondys, y_nondys, cv=cv, scoring=scoring, n_jobs=-1)\n",
        "print(\"Non-dyslexic (5-fold):\",\n",
        "      f\"Acc {np.mean(res_nondys['test_acc']):.3f}\",\n",
        "      f\"F1 {np.mean(res_nondys['test_f1']):.3f}\",\n",
        "      f\"ROC {np.mean(res_nondys['test_roc']):.3f}\",\n",
        "      f\"PR {np.mean(res_nondys['test_pr']):.3f}\")\n",
        "\n",
        "\n",
        "def mean_std(arr):\n",
        "    return f\"{np.mean(arr):.3f} ± {np.std(arr):.3f}\"\n",
        "\n",
        "print(\"Dyslexic (5-fold):\",\n",
        "      f\"Acc {mean_std(res_dys['test_acc'])}\",\n",
        "      f\"F1 {mean_std(res_dys['test_f1'])}\",\n",
        "      f\"ROC {mean_std(res_dys['test_roc'])}\",\n",
        "      f\"PR {mean_std(res_dys['test_pr'])}\")\n",
        "\n",
        "print(\"Non-dyslexic (5-fold):\",\n",
        "      f\"Acc {mean_std(res_nondys['test_acc'])}\",\n",
        "      f\"F1 {mean_std(res_nondys['test_f1'])}\",\n",
        "      f\"ROC {mean_std(res_nondys['test_roc'])}\",\n",
        "      f\"PR {mean_std(res_nondys['test_pr'])}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "niWQ2MP31bnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferential\n",
        "dys_ratings = word[word['dyslexia']==1]\n",
        "nondys_ratings = word[word['dyslexia']==0]\n",
        "nondys_ratings['Prolific ID'].nunique(), dys_ratings[\"Prolific ID\"].nunique()\n",
        "\n",
        "dys_ratings = word[word['dyslexia']==1]\n",
        "nondys_ratings = word[word['dyslexia']==0]\n",
        "nondys_ratings['Prolific ID'].nunique(), dys_ratings[\"Prolific ID\"].nunique()\n",
        "\n",
        "\n",
        "features = [\n",
        "    'long_words', 'diagraphs', 'not_freq_words',\n",
        "    'silent_letters', 'vowel_digraphs', 'has_homophones',\n",
        "    'difficult_orthography'\n",
        "]\n",
        "\n",
        "df = word.copy()\n",
        "\n",
        "\n",
        "df['last_difficulty_num'] = pd.to_numeric(df['last_difficulty'].astype(str).str.strip(), errors='coerce')\n",
        "df = df[df['last_difficulty_num'].isin([1,2,3,4])]\n",
        "\n",
        "\n",
        "df['binary'] = np.where(df['last_difficulty_num'] == 1, 'Very Easy', 'Not Very Easy')\n",
        "\n",
        "df[features] = df[features].fillna(0).astype(int)\n",
        "\n",
        "results = []\n",
        "\n",
        "for feat in features:\n",
        "    df_feat = df[df[feat] == 1].copy()\n",
        "\n",
        "    table = pd.crosstab(df_feat['dyslexia'], df_feat['binary'])\n",
        "    table = table.reindex(index=[True, False], columns=['Not Very Easy', 'Very Easy'], fill_value=0)\n",
        "\n",
        "    # Fisher exact needs\n",
        "    odds_ratio, p_value = fisher_exact(table.values)\n",
        "\n",
        "\n",
        "    # Compare odds of Not Very Easy (dys vs non)\n",
        "\n",
        "    a, b = table.loc[True,  'Not Very Easy'], table.loc[True,  'Very Easy']\n",
        "    c, d = table.loc[False, 'Not Very Easy'], table.loc[False, 'Very Easy']\n",
        "\n",
        "    dys_odds = (a / b) if b != 0 else np.inf\n",
        "    non_odds = (c / d) if d != 0 else np.inf\n",
        "    direction = \"Dys higher Not Very Easy\" if dys_odds > non_odds else \"Non higher Not Very Easy\"\n",
        "\n",
        "    results.append({\n",
        "        \"feature\": feat,\n",
        "        \"dys_NotVeryEasy\": int(a),\n",
        "        \"dys_VeryEasy\": int(b),\n",
        "        \"non_NotVeryEasy\": int(c),\n",
        "        \"non_VeryEasy\": int(d),\n",
        "        \"odds_ratio\": odds_ratio,\n",
        "        \"p_raw\": p_value,\n",
        "        \"direction\": direction\n",
        "    })\n",
        "\n",
        "res = pd.DataFrame(results)\n",
        "\n",
        "# FDR correction\n",
        "res[\"p_fdr\"] = multipletests(res[\"p_raw\"], method=\"fdr_bh\")[1]\n",
        "res[\"significant_fdr_0.05\"] = res[\"p_fdr\"] < 0.05\n",
        "\n",
        "# sort by p\n",
        "res = res.sort_values(\"p_fdr\")\n",
        "res\n"
      ],
      "metadata": {
        "id": "ABXAWJl_1bkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = [\n",
        "    'long_words', 'diagraphs', 'not_freq_words',\n",
        "    'silent_letters', 'vowel_digraphs', 'has_homophones',\n",
        "    'difficult_orthography'\n",
        "]\n",
        "\n",
        "df = word.copy()\n",
        "\n",
        "df['last_difficulty_num'] = pd.to_numeric(df['last_difficulty'].astype(str).str.strip(), errors='coerce')\n",
        "df = df[df['last_difficulty_num'].isin([1,2,3,4])]\n",
        "\n",
        "df['binary'] = np.where(df['last_difficulty_num'] == 1, 'Very Easy', 'Not Very Easy')\n",
        "df[features] = df[features].fillna(0).astype(int)\n",
        "\n",
        "results = []\n",
        "\n",
        "Z = 1.96\n",
        "\n",
        "for feat in features:\n",
        "    df_feat = df[df[feat] == 1].copy()\n",
        "\n",
        "    table = pd.crosstab(df_feat['dyslexia'], df_feat['binary'])\n",
        "    table = table.reindex(index=[True, False], columns=['Not Very Easy', 'Very Easy'], fill_value=0)\n",
        "\n",
        "    # counts\n",
        "    a = int(table.loc[True,  'Not Very Easy'])\n",
        "    b = int(table.loc[True,  'Very Easy'])\n",
        "    c = int(table.loc[False, 'Not Very Easy'])\n",
        "    d = int(table.loc[False, 'Very Easy'])\n",
        "\n",
        "    # fisher exact p-value + OR\n",
        "    odds_ratio, p_value = fisher_exact([[a, b], [c, d]])\n",
        "\n",
        "    if b == 0 or d == 0:\n",
        "        dys_odds = (a + 0.5) / (b + 0.5)\n",
        "        non_odds = (c + 0.5) / (d + 0.5)\n",
        "    else:\n",
        "        dys_odds = a / b\n",
        "        non_odds = c / d\n",
        "    direction = \"Dys higher Not Very Easy\" if dys_odds > non_odds else \"Non higher Not Very Easy\"\n",
        "\n",
        "\n",
        "    if (a == 0) or (b == 0) or (c == 0) or (d == 0):\n",
        "        a_ci, b_ci, c_ci, d_ci = a + 0.5, b + 0.5, c + 0.5, d + 0.5\n",
        "    else:\n",
        "        a_ci, b_ci, c_ci, d_ci = float(a), float(b), float(c), float(d)\n",
        "\n",
        "    # log(OR) and SE\n",
        "    or_ci = (a_ci * d_ci) / (b_ci * c_ci)\n",
        "    se = np.sqrt(1/a_ci + 1/b_ci + 1/c_ci + 1/d_ci)\n",
        "    log_or = np.log(or_ci)\n",
        "\n",
        "    ci_low = np.exp(log_or - Z * se)\n",
        "    ci_high = np.exp(log_or + Z * se)\n",
        "\n",
        "    results.append({\n",
        "        \"feature\": feat,\n",
        "        \"dys_NotVeryEasy\": a,\n",
        "        \"dys_VeryEasy\": b,\n",
        "        \"non_NotVeryEasy\": c,\n",
        "        \"non_VeryEasy\": d,\n",
        "        \"odds_ratio\": odds_ratio,\n",
        "        \"ci95_low\": ci_low,\n",
        "        \"ci95_high\": ci_high,\n",
        "        \"p_raw\": p_value,\n",
        "        \"direction\": direction\n",
        "    })\n",
        "\n",
        "res = pd.DataFrame(results)\n",
        "\n",
        "# fdr correction\n",
        "res[\"p_fdr\"] = multipletests(res[\"p_raw\"], method=\"fdr_bh\")[1]\n",
        "res[\"significant_fdr_0.05\"] = res[\"p_fdr\"] < 0.05\n",
        "\n",
        "res = res.sort_values(\"p_fdr\")\n",
        "\n",
        "res\n"
      ],
      "metadata": {
        "id": "8JEd4RQT1bhv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_fmt = res.copy()\n",
        "\n",
        "res_fmt[\"OR\"] = res_fmt[\"odds_ratio\"].map(lambda x: f\"{x:.2f}\")\n",
        "res_fmt[\"95% CI\"] = res_fmt.apply(lambda r: f\"[{r['ci95_low']:.2f}, {r['ci95_high']:.2f}]\", axis=1)\n",
        "\n",
        "def fmt_p(p):\n",
        "    return \"< .001\" if p < 1e-3 else f\"{p:.3f}\"\n",
        "res_fmt[\"p (FDR)\"] = res_fmt[\"p_fdr\"].map(fmt_p)\n",
        "\n",
        "res_fmt[\"Dys (NVE/VE)\"] = res_fmt.apply(lambda r: f\"{int(r['dys_NotVeryEasy'])}/{int(r['dys_VeryEasy'])}\", axis=1)\n",
        "res_fmt[\"Non (NVE/VE)\"] = res_fmt.apply(lambda r: f\"{int(r['non_NotVeryEasy'])}/{int(r['non_VeryEasy'])}\", axis=1)\n",
        "\n",
        "final_table = res_fmt[[\"feature\", \"Dys (NVE/VE)\", \"Non (NVE/VE)\", \"OR\", \"95% CI\", \"p (FDR)\"]]\n",
        "final_table"
      ],
      "metadata": {
        "id": "w5x_5yG52oQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# teachers data\n",
        "\n",
        "teachers_words = gc.open_by_url('https://docs.google.com/spreadsheets/d/1BgHVgMfZjQkiA8rJLfuPlXGCJ6S5w138IlO-0kCS0SA/edit?gid=0#gid=0')\n",
        "teachers_pre = gc.open_by_url('https://docs.google.com/spreadsheets/d/1XoA_8w8mI5LdhD8sIahDba2_OhdpGZZQvmLzl7sq1Cw/edit?gid=1328307878#gid=1328307878')\n",
        "teachers_post = gc.open_by_url('https://docs.google.com/spreadsheets/d/1o3Tz4DAnyh7__GsXxM0Ir0zvFR60rjicwibX10-kwxE/edit?gid=575842559#gid=575842559')\n",
        "\n",
        "wordsheet = teachers_words.worksheet('Sheet1')\n",
        "presheet = teachers_pre.worksheet('Sheet1')\n",
        "postsheet = teachers_post.worksheet('Sheet1')\n",
        "\n",
        "tword = get_as_dataframe(wordsheet)\n",
        "tpre = get_as_dataframe(presheet)\n",
        "tpost = get_as_dataframe(postsheet)\n",
        "\n",
        "tword = tword.drop(columns={'Unnamed: 10','Unnamed: 11'})\n",
        "tword.shape, tpre.shape, tpost.shape, tword.columns\n",
        "\n",
        "# Teacher - pre processing\n",
        "def safe_zipf(word):\n",
        "    if isinstance(word, str):\n",
        "        return zipf_frequency(word, 'en')\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "tpre = tpre.rename(columns={'Please enter your Prolific ID:':'Prolific ID'})\n",
        "tpre = tpre.drop_duplicates(subset='Prolific ID')\n",
        "duplicate_ids = tpre['Prolific ID'][tpre['Prolific ID'].duplicated()]\n",
        "print(tword.columns)\n",
        "tpre = tpre[tpre['grade category'].isna()== False]\n",
        "\n",
        "tword = tword.rename(columns={'word':'words'})\n",
        "\n",
        "tword['last_difficulty'] = tword['difficulty'].fillna('').astype(str).str.split(',').str[-1]\n",
        "\n",
        "tword['zipf'] = tword['words'].apply(safe_zipf)\n",
        "\n",
        "tword = tword.drop(columns=['difficulty', 'timeStamps', 'reason', 'url', 'Server Timestamp'])\n",
        "tword.columns"
      ],
      "metadata": {
        "id": "zgQ9OVDk2oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# teachers - data consistency\n",
        "tpre['What is your gender?'] = tpre['What is your gender?'].str.strip().str.lower()\n",
        "\n",
        "gender_map = {\n",
        "    'nb': 'unknown',\n",
        "    'chick': 'unknown',\n",
        "    'male': 'Male',\n",
        "    'female': 'Female',\n",
        "    'man': 'Male',\n",
        "    'woman': 'Female',\n",
        "    'nonbinary': 'Non-binary',\n",
        "    'non-binary': 'Non-binary',\n",
        "    'transgender': 'Transgender',\n",
        "    'm':'Male',\n",
        "    'femal':'Female',\n",
        "    'female/ woman' : 'Female',\n",
        "    'F':'Female',\n",
        "    'femaile':'Female',\n",
        "    'fEMALE':'Female',\n",
        "    'Email' : 'Female',\n",
        "    'f': 'Female','email': 'Female'\n",
        "}\n",
        "\n",
        "tpre['What is your gender?'] = tpre['What is your gender?'].map(gender_map).fillna(tpre['What is your gender?'])\n",
        "\n",
        "tpre['What is your gender?'].value_counts()\n",
        "\n",
        "# Teachers - data integration (pre-survey and word dataset)\n",
        "print(tpre['Prolific ID'].nunique(), tword['Prolific ID'].nunique())\n",
        "\n",
        "tpre_list = tpre['Prolific ID'].to_list()\n",
        "tword_list = tword['Prolific ID'].to_list()\n",
        "\n",
        "temp = tword[~tword['Prolific ID'].isin(tpre_list)]\n",
        "print(temp['Prolific ID'].nunique()) # must be 0\n",
        "\n",
        "tpre = tpre[tpre['Prolific ID'].isin(tword_list)]\n",
        "print(tpre['Prolific ID'].nunique())\n",
        "\n",
        "#mean, min, max\n",
        "tpre['What is your age? (in years)'] = tpre['What is your age? (in years)'].astype(int)\n",
        "tpre = tpre[tpre['What is your age? (in years)'] != 0]\n",
        "\n",
        "tpre1 = tpre[tpre['What is your gender?'].isin(['Male', 'Female'])]\n",
        "tpre1['What is your age? (in years)'].astype(int).describe()\n"
      ],
      "metadata": {
        "id": "Zci_AUue22D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Teachers- Quartile-Based Adjustment:\n",
        "\n",
        "tword['date and time'] = pd.to_datetime(tword['date and time'])\n",
        "\n",
        "counts = tword.groupby('Prolific ID').size()\n",
        "valid_ids = counts[counts <= 55].index\n",
        "filtered = tword[tword['Prolific ID'].isin(valid_ids)]\n",
        "\n",
        "only_1_2_ids = filtered.groupby('Prolific ID')['last_difficulty'].apply(\n",
        "    lambda x: x.astype(str).str.strip().isin(['1','2']).all()\n",
        ")\n",
        "only_1_2_ids = only_1_2_ids[only_1_2_ids].index\n",
        "print('len', len(only_1_2_ids))\n",
        "\n",
        "\n",
        "durations = filtered.groupby('Prolific ID')['date and time'].max() - filtered.groupby('Prolific ID')['date and time'].min()\n",
        "durations_seconds = durations.dt.total_seconds()\n",
        "\n",
        "careless_durations = durations_seconds.loc[only_1_2_ids]\n",
        "\n",
        "# Compute cutoff = 25th percentile\n",
        "quartiles = careless_durations.quantile([0.25, 0.5, 0.75])\n",
        "cutoff = careless_durations.quantile(0.25)\n",
        "\n",
        "print(\"Quartiles of their session durations (seconds):\")\n",
        "print(quartiles)\n",
        "print(\"\\nCutoff (25th percentile of careless durations):\", cutoff, \"seconds ≈\", cutoff/60, \"minutes\")\n",
        "print(careless_durations.sort_values())\n",
        "\n",
        "df = tword.groupby('Prolific ID')['last_difficulty']\n",
        "all_ones = df.apply(lambda s: (s == '1').all())\n",
        "all_ones.value_counts()\n",
        "num_participants_all_1 = int(all_ones.sum())\n",
        "ids_all_1 = all_ones[all_ones].index.tolist()\n",
        "\n",
        "\n",
        "tword = tword[~tword['Prolific ID'].isin(ids_all_1)].copy()\n",
        "\n",
        "# remove the unreliable participants just selected 1 and 2 and did not take enough time\n",
        "df1 = tword.copy()\n",
        "df1['last_difficulty'] = pd.to_numeric(df1['last_difficulty'], errors='coerce')\n",
        "df1['date and time'] = pd.to_datetime(df1['date and time'], errors='coerce')\n",
        "df1 = df1.dropna(subset=['last_difficulty', 'date and time'])\n",
        "\n",
        "df = df1.groupby('Prolific ID')\n",
        "\n",
        "only_12 = df['last_difficulty'].apply(lambda s: s.isin([1,2]).all())\n",
        "dur_min = (df['date and time'].max() - df['date and time'].min()).dt.total_seconds() / 60\n",
        "ids_3 = dur_min.index[only_12 & (dur_min < cutoff)].tolist()\n",
        "\n",
        "\n",
        "print('len', tword['Prolific ID'].nunique() )\n",
        "\n",
        "tword = tword[~tword['Prolific ID'].isin(ids_3)].copy()\n",
        "print(tword['Prolific ID'].nunique())\n",
        "\n",
        "# teacher's - XGBoost\n",
        "\n",
        "# tword['hard'] = [0 if tword['last_difficulty'][i] == '1' else 1 for i in range(len(tword))]\n",
        "# tword= tword.drop(columns=['Prolific ID', 'words', 'letters', 'reason explanation', 'date and time', 'last_difficulty' ])\n",
        "X_teacher = tword.drop(columns=['hard'])\n",
        "y_teacher = tword['hard']\n",
        "\n",
        "\n",
        "scoring = {\n",
        "    \"acc\": \"accuracy\",\n",
        "    \"f1\": \"f1\",\n",
        "    \"roc\": \"roc_auc\",\n",
        "    \"pr\": \"average_precision\",\n",
        "}\n",
        "\n",
        "neg, pos = np.bincount(y_teacher)\n",
        "scale_pos_weight = neg / pos\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "clf = xgb.XGBClassifier(\n",
        "    tree_method=\"hist\",\n",
        "    eval_metric=\"logloss\",\n",
        "    random_state=42,\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=4,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=1.0,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "teacher_res = cross_validate(clf, X_teacher, y_teacher, cv=cv, scoring=scoring, n_jobs=-1)\n",
        "print(\"teachers (5-fold):\",\n",
        "      f\"Acc {np.mean(teacher_res['test_acc']):.3f}\",\n",
        "      f\"F1 {np.mean(teacher_res['test_f1']):.3f}\",\n",
        "      f\"ROC {np.mean(teacher_res['test_roc']):.3f}\",\n",
        "      f\"PR {np.mean(teacher_res['test_pr']):.3f}\")\n",
        "\n",
        "\n",
        "def mean_std(arr):\n",
        "    return f\"{np.mean(arr):.3f} ± {np.std(arr):.3f}\"\n",
        "\n",
        "print(\"Teachers (5-fold):\",\n",
        "      f\"Acc {mean_std(teacher_res['test_acc'])}\",\n",
        "      f\"F1 {mean_std(teacher_res['test_f1'])}\",\n",
        "      f\"ROC {mean_std(teacher_res['test_roc'])}\",\n",
        "      f\"PR {mean_std(teacher_res['test_pr'])}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "B4a8BWIX22BG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}